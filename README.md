# ViT + GPT2 Image Captioning Project

## ğŸ“ Project Structure

```
image_captioning/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ results.csv         # CSV file: image_name, comment
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ flickr_dataset.py    # Dataset class with transform, padding, and tokenizer
â”œâ”€â”€ models/
â”‚   â””â”€â”€ vit_gpt2.py           # ViT-GPT2 model integration
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # Main training script
â”‚   â””â”€â”€ metrics.py            # BLEU, ROUGE, METEOR, Cosine similarity
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ device.py            # CPU/GPU detection
â”‚   â”œâ”€â”€ save.py              # Save best model and remove old ones
â”‚   â””â”€â”€ plot.py              # Plot training loss and metrics
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ infer.py             # Generate captions using trained model
â””â”€â”€ README.md
```

---

## ğŸš€ Features

- **Transformer-based architecture**: Vision Transformer (ViT) encoder + GPT2 decoder
- **Full metrics**: BLEU, ROUGE, METEOR, Cosine Similarity
- **Cosine warmup scheduler**: With good initial learning rate and decay
- **Robust training loop**: Early stopping + best checkpoint saving
- **Device-aware**: GPU/MPS/CPU auto-selection
- **Clean visualization**: Matplotlib plots for all training stats
- **Modular design**: Easy to debug, extend or replace modules

---

## âš–ï¸ How to Run

### 1. Setup

```bash
pip install -r requirements.txt
```

### 2. Prepare Data

- Place all Flickr30k images in `data/images/`
- Ensure `results.csv` is in `data/` with format:
  ```csv
  image_name,comment
  123.jpg,A child playing in the park.
  ```

### 3. Train Model

```bash
python training/train.py
```

### 4. Inference

```python
from inference.infer import generate_caption
caption = generate_caption("data/images/sample.jpg", "checkpoints/best_model.pt")
print(caption)
```

### 5. Plot Metrics

```bash
python utils/plot.py
```

---

## ğŸ† Results

- Automatically saves:
  - `loss.png` and `metrics.png`
  - `checkpoints/best_model.pt`
- Cleaned intermediate model files

---

## ğŸ”§ Customization

- **Hyperparameters** in `train.py`
- **Tokenizer/model** from HuggingFace (`vit-base-patch16-224`, `gpt2`)
- **Max caption length, padding** in `flickr_dataset.py`

---

## ğŸšœ Roadmap

-

---

## âœ¨ Credits

- Vision Transformer: [https://huggingface.co/google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
- GPT2 Language Model: [https://huggingface.co/gpt2](https://huggingface.co/gpt2)
- Flickr30k Dataset: [https://shannon.cs.illinois.edu/DenotationGraph/](https://shannon.cs.illinois.edu/DenotationGraph/)

---

## ğŸ“¢ Issues / Help?

Please open a GitHub issue or ping me with your questions!

